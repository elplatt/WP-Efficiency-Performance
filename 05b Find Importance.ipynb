{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from lxml import html\n",
    "from multiprocessing import Process, Queue\n",
    "from Queue import Empty\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from urlparse import parse_qs, urlparse\n",
    "import logbook\n",
    "from IPython.display import clear_output\n",
    "\n",
    "orig_stderr = sys.stderr\n",
    "sys.stderr = open(\"/dev/null\", \"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensure_unicode(s):\n",
    "    if isinstance(s, unicode):\n",
    "        return s\n",
    "    else:\n",
    "        return s.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp = logbook.Experiment(\"05b_find_importance\")\n",
    "log = exp.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = requests.get(\"https://tools.wmflabs.org/enwp10/cgi-bin/pindex.fcgi?sec=[All]\")\n",
    "parser = html.HTMLParser(encoding='utf-8')\n",
    "tree = html.document_fromstring(page.content, parser=parser)\n",
    "rows = tree.xpath(\"//table[@class='wikitable']//tr\")\n",
    "\n",
    "log.info(\"Parsing projects\")\n",
    "projects = []\n",
    "for i, row in enumerate(rows):\n",
    "    cells = row.xpath(\"td\")\n",
    "    if len(cells) < 3:\n",
    "        continue\n",
    "    # Parse project title\n",
    "    if (len(cells[0][0]) == 0):\n",
    "        title = ensure_unicode(cells[0][0].text)\n",
    "        project_title = title\n",
    "        project_unique = title\n",
    "    else:\n",
    "        title = ensure_unicode(cells[0][0][0].text)\n",
    "        try:\n",
    "            url = cells[0][0][0].attrib['href']\n",
    "            query = urlparse(url).query\n",
    "            unique = parse_qs(query)['title'][0].decode('utf8')\n",
    "            project_title = title\n",
    "            project_unique = unique\n",
    "        except KeyError:\n",
    "            # No title\n",
    "            project_title = title\n",
    "            project_unique = title\n",
    "    # Parse list url\n",
    "    project_list = cells[2].xpath(\"a[1]\")[0].attrib['href']\n",
    "    projects.append( (project_title, project_unique, project_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse articles\n",
    "log.info(\"Parsing project articles\")\n",
    "try:\n",
    "    with open(exp.get_filename(\"importance_url.utf8.tsv\"), \"wb\") as out:\n",
    "        out.write(\"proj_title\\tproj_unique\\tpage_url\\timportance\\n\")\n",
    "        articles = [] # [project_title, project_unique, article_url, importance]\n",
    "        for project in projects:\n",
    "            # Get list of articles in project\n",
    "            query = urlparse(project[2]).query\n",
    "            project_query = parse_qs(query)['project'][0]\n",
    "            url = \"https://tools.wmflabs.org/enwp10/cgi-bin/list2.fcgi?run=yes&projecta=%s&namespace=&pagename=&quality=&importance=&score=&limit=250&offset=1&sorta=Importance&sortb=Quality\" % project_query\n",
    "            page = requests.get(url)\n",
    "            parser = html.HTMLParser(encoding='utf-8')\n",
    "            tree = html.document_fromstring(page.content, parser=parser)\n",
    "            rows = tree.xpath(\"//table[@class='wikitable']//tr\")\n",
    "            if len(rows) == 0:\n",
    "                print \"No rows in \" + project[0]\n",
    "                continue\n",
    "            for row in rows:\n",
    "                cells = row.xpath(\"td\")\n",
    "                try:\n",
    "                    article_href = cells[1][0].attrib['href']\n",
    "                    article_name = cells[2][0].text\n",
    "                    article_data = [project[0], project[1], article_href, article_name]\n",
    "#                    out.write(\"\\t\".join([\n",
    "#                        article_data[0].encode('utf8'),\n",
    "#                        article_data[1].encode('utf8'),\n",
    "#                        article_data[2].encode('utf8'),\n",
    "#                        article_data[3].encode('utf8')\n",
    "#                    ]) + \"\\n\")\n",
    "#                    articles.append(article_data)\n",
    "                    try:\n",
    "                        # Get talk link if it exists\n",
    "                        links = cells[1].xpath(\"a\")\n",
    "                        talk_href = links[1].attrib['href']\n",
    "                        talk_data = [project[0], project[1], talk_href, article_name]\n",
    "                        out.write(\"\\t\".join([\n",
    "                            talk_data[0].encode('utf8'),\n",
    "                            talk_data[1].encode('utf8'),\n",
    "                            talk_data[2].encode('utf8'),\n",
    "                            talk_data[3].encode('utf8')\n",
    "                        ]) + \"\\n\")\n",
    "                        articles.append(talk_data)\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                finally:\n",
    "                    out.flush()\n",
    "except:\n",
    "    log.error(sys.exc_info())\n",
    "    traceback.print_exc(file=open(exp.get_filename(\"error.txt\"), \"wb\"))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Alternative: Load articles from file\n",
    "articles = []\n",
    "article_file = \"output/04b_find_importance/2017-09-15 15:01:52 14665f3/importance_url.utf8.tsv\"\n",
    "log.info(\"Loading articles\")\n",
    "finished = 0\n",
    "with open(article_file, \"rb\") as f:\n",
    "    f.next()\n",
    "    for i, row_bytes in enumerate(f):\n",
    "        row = row_bytes.decode('utf-8')\n",
    "        article_data = row.strip().split(u\"\\t\")\n",
    "        articles.append(article_data)\n",
    "        finished += 1\n",
    "print len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_id_from_url(article):\n",
    "    article = list(article)\n",
    "    query = urlparse(article[2]).query\n",
    "    article_title = parse_qs(query)['title'][0]\n",
    "    info_url = \"https://en.wikipedia.org/w/index.php?title=%s&action=info\" % article_title\n",
    "    page = requests.get(info_url)\n",
    "    parser = html.HTMLParser(encoding='utf-8')\n",
    "    tree = html.document_fromstring(page.content, parser=parser)\n",
    "    cell = tree.xpath(\"//tr[@id='mw-pageinfo-article-id']//td\")[1]\n",
    "    page_id = int(cell.text.strip())\n",
    "    article[2] = page_id\n",
    "    del tree\n",
    "    return article\n",
    "\n",
    "def worker(worker_id, article_q, result_q, skipped_q, error_q, done_q):\n",
    "    loop_num = 0\n",
    "    try:\n",
    "        while True:\n",
    "            loop_num += 1\n",
    "            if loop_num % 200 == 0:\n",
    "                time.sleep(1)\n",
    "            try:\n",
    "                article = article_q.get(False, 1.0)\n",
    "            except Empty:\n",
    "                if article_q.qsize() > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            try:\n",
    "                new_article = get_id_from_url(article)\n",
    "            except IndexError:\n",
    "                skipped_q.put(article)\n",
    "                continue\n",
    "            result_q.put(new_article)\n",
    "    except Empty:\n",
    "        print \"Queue size is \", article_q.qsize()\n",
    "    except:\n",
    "        # Unknown error, push to queue and empty input queue\n",
    "        exc = \"\".join(traceback.format_exception(sys.last_type, sys.last_value, sys.last_traceback))\n",
    "        error_q.put(exc)\n",
    "        while True:\n",
    "            try:\n",
    "                article_q.get(False)\n",
    "            except Empty:\n",
    "                break\n",
    "    done_q.put(worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Replace url with page id\n",
    "num_workers = 5\n",
    "article_q = Queue()\n",
    "result_q = Queue()\n",
    "error_q = Queue()\n",
    "skipped_q = Queue()\n",
    "done_q = Queue()\n",
    "log_every = 200\n",
    "next_log = log_every\n",
    "num_articles = len(articles)\n",
    "log.info(\"Replacing url with ids\")\n",
    "try:\n",
    "    articles_put = 0\n",
    "    for i, article in enumerate(articles):\n",
    "        article_q.put(article)\n",
    "        articles_put += 1\n",
    "    log.info(\"  Put %d articles\" % articles_put)\n",
    "    workers = []\n",
    "    log.info(\"  %d Articles to process\" % article_q.qsize())\n",
    "    log.info(\"  Starting workers\")\n",
    "    for i in range(num_workers):\n",
    "        w = Process(target=worker, args=(i, article_q, result_q, skipped_q, error_q, done_q))\n",
    "        w.daemon = True\n",
    "        workers.append(w)\n",
    "        w.start()\n",
    "    log.info(\"  Workers started\")\n",
    "    while result_q.qsize() + skipped_q.qsize() < len(articles):\n",
    "        time.sleep(0)\n",
    "        if error_q.qsize() == 0:\n",
    "            if result_q.qsize() > next_log:\n",
    "                next_log += log_every\n",
    "                log.info(\"  %d/%d articles complete (%d skipped)\" % (result_q.qsize(), num_articles, skipped_q.qsize()))\n",
    "        else:\n",
    "            # Error occurred, empty queues\n",
    "            log.error(error_q.get())\n",
    "            try:\n",
    "                while True:\n",
    "                    result_q.get()\n",
    "            except Empty:\n",
    "                pass\n",
    "    if error_q.qsize() > 0:\n",
    "        log.error(error_q.get())\n",
    "    log.info(\"Completed %d articles, skipped %d\" % (result_q.qsize(), skipped_q.qsize()))\n",
    "except:\n",
    "    log.error(sys.exc_info())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp.get_filename(\"\"), result_q.qsize(), skipped_q.qsize(), article_q.qsize(), done_q.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        article = result_q.get(False, 10.0)\n",
    "        id_articles.append(article)\n",
    "    except Empty:\n",
    "        clear_output()\n",
    "        print len(id_articles)\n",
    "        if result_q.qsize() == 0:\n",
    "            break\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(id_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(exp.get_filename(\"importance.utf8.tsv\"), \"wb\") as out:\n",
    "    out.write(\"proj_title\\tproj_unique\\tpage_id\\timportance\\n\")\n",
    "    for article in id_articles:\n",
    "        out.write(\"\\t\".join([\n",
    "            article[0].encode('utf8'),\n",
    "            article[1].encode('utf8'),\n",
    "            str(article[2]),\n",
    "            article[3].encode('utf8')\n",
    "        ]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "492650 - 488935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
