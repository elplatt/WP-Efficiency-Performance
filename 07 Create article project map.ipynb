{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import codecs\n",
    "import datetime\n",
    "import dateutil\n",
    "import json\n",
    "import sys\n",
    "import msgpack\n",
    "from sqlalchemy import and_, select\n",
    "import database\n",
    "import logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_name = \"07_create_article_project_map\"\n",
    "project_file = \"data/projects-2016-10-14.json\"\n",
    "skip_assess_file = \"skipped_assessments.tsv\"\n",
    "skip_timespan_file = \"skipped_timespans.tsv\"\n",
    "out_file = \"articles_projects.m\"\n",
    "dump_dt = 1459468800 # \"2016-04-01T00:00:00Z\"\n",
    "exp = logbook.Experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_names = {}\n",
    "with open(project_file, \"rb\") as f:\n",
    "    for row in f:\n",
    "        d = json.loads(row)\n",
    "        project_names[d[\"project_id\"]] = d[\"project_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_article_dates(project_id, conn, skipped):\n",
    "    '''Return list of (article_name_utf8, start_time, end_time).'''\n",
    "    tab = database.schema.Rating.__table__\n",
    "    stmt = select([\n",
    "            tab.c.action,\n",
    "            tab.c.timestamp,\n",
    "            tab.c.article_name,\n",
    "            tab.c.new_article_name]) \\\n",
    "        .where(tab.c.project_id == project_id) \\\n",
    "        .order_by(tab.c.timestamp)\n",
    "    result = conn.execute(stmt)\n",
    "    article_start = {}\n",
    "    names_dates = []\n",
    "    for row in result:\n",
    "        action_utf8, timestamp, article_name_utf8, new_article_name_utf8 = row\n",
    "        talk_name_utf8 = \"Talk:%s\" % article_name_utf8\n",
    "        new_talk_name_utf8 = \"Talk:%s\" % article_name_utf8\n",
    "        if action_utf8 == \"Renamed\":\n",
    "            # Track the article's name change\n",
    "            try:\n",
    "                # Get timestamp for start of previous name\n",
    "                start = article_start[article_name_utf8]\n",
    "                del article_start[article_name_utf8]\n",
    "            except KeyError:\n",
    "                skipped.write(u\"\\t\".join([str(x).decode('utf-8') for x in row]) + u\"\\n\")\n",
    "                continue\n",
    "            names_dates.append( (article_name_utf8, start, timestamp) )\n",
    "            article_start[new_article_name_utf8] = timestamp\n",
    "            # Also track talk page\n",
    "            try:\n",
    "                # Get timestamp for start of previous name\n",
    "                talk_start = article_start[talk_name_utf8]\n",
    "                del article_start[talk_name_utf8]\n",
    "            except KeyError:\n",
    "                talk_data = [action_utf8, timestamp, talk_name_utf8, new_talk_name_utf8]\n",
    "                skipped.write(u\"\\t\".join([str(x).decode('utf-8') for x in talk_data]) + u\"\\n\")\n",
    "            names_dates.append( (talk_name_utf8, start, timestamp) )\n",
    "            article_start[new_talk_name_utf8] = timestamp\n",
    "        elif action_utf8 == \"Removed\":\n",
    "            # Track article removal\n",
    "            try:\n",
    "                # Get timestamp for start of previous name\n",
    "                start = article_start[article_name_utf8]\n",
    "                del article_start[article_name_utf8]\n",
    "            except KeyError:\n",
    "                skipped.write(u\"\\t\".join([str(x).decode('utf8') for x in row]) + u\"\\n\")\n",
    "                continue\n",
    "            names_dates.append( (article_name_utf8, start, timestamp) )\n",
    "            # Also track talk page\n",
    "            try:\n",
    "                # Get timestamp for start of previous name\n",
    "                talk_start = article_start[talk_name_utf8]\n",
    "                del article_start[talk_name_utf8]\n",
    "            except KeyError:\n",
    "                talk_data = [action_utf8, timestamp, talk_name_utf8, new_talk_name_utf8]\n",
    "                skipped.write(u\"\\t\".join([str(x).decode('utf-8') for x in talk_data]) + u\"\\n\")\n",
    "            names_dates.append( (talk_name_utf8, start, timestamp) )\n",
    "        elif article_name_utf8 not in article_start:\n",
    "            article_start[article_name_utf8] = timestamp\n",
    "            article_start[talk_name_utf8] = timestamp\n",
    "    # If articles are started but not renamed or removed, use the dump date\n",
    "    for article_name_utf8, start in article_start.iteritems():\n",
    "        names_dates.append( (article_name_utf8, start, dump_dt) )\n",
    "    return names_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_id_dates(names_dates, conn, skipped):\n",
    "    '''Get article_id for each article_name and timespan.\n",
    "    Returns {id => [from_ts1, to_ts1, from_ts2, from_ts2, ...]}.\n",
    "    '''\n",
    "    tab = database.schema.article_name_id\n",
    "    id_dates = {}\n",
    "    for row in names_dates:\n",
    "        article_name_utf8, from_ts, to_ts = row\n",
    "        # Use midpoint to look up id in case ends are mismatched\n",
    "        mid_ts = (from_ts + to_ts) / 2\n",
    "        mid_dt = datetime.datetime.utcfromtimestamp(mid_ts)\n",
    "        stmt = select([tab.c.article_id]) \\\n",
    "            .where(and_(\n",
    "                tab.c.article_name == article_name_utf8,\n",
    "                tab.c.from_ts < mid_dt,\n",
    "                tab.c.to_ts > mid_dt))\n",
    "        result = conn.execute(stmt)\n",
    "        try:\n",
    "            article_id = result.fetchone()[0]\n",
    "        except TypeError:\n",
    "            skipped.write(u\"\\t\".join([str(x).decode('utf-8') for x in row]) + u\"\\n\")\n",
    "            continue\n",
    "        try:\n",
    "            id_dates[article_id].append(from_ts)\n",
    "            id_dates[article_id].append(to_ts)\n",
    "        except KeyError:\n",
    "            id_dates[article_id] = [from_ts, to_ts]\n",
    "    return id_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log = exp.get_logger()\n",
    "log.info(\"Beginning\")\n",
    "skipped_assess = codecs.open(exp.get_filename(skip_assess_file), \"w\", encoding=\"utf-16-le\")\n",
    "skipped_timespan = codecs.open(exp.get_filename(skip_timespan_file), \"w\", encoding=\"utf-16-le\")\n",
    "conn = database.engine.connect()\n",
    "try:\n",
    "    #id_projects = {article_id: { project_id: [start, stop] } }\n",
    "    id_projects = {}\n",
    "    # Iterate through projects\n",
    "    for project_id in project_names.iterkeys():\n",
    "        log.info(u\"Project %d: %s\" % (project_id, project_names[project_id]))\n",
    "        log.info(\"  Finding timespans\")\n",
    "        # Get all article names in the project and the timespans they were in the project\n",
    "        # with that name\n",
    "        names_dates = get_article_dates(project_id, conn, skipped_assess)\n",
    "        log.info(\"  Finding ids\")\n",
    "        # Get article ids and the list of timespans they were in the project\n",
    "        id_dates = get_id_dates(names_dates, conn, skipped_timespan)\n",
    "        log.info(\"  Combining ids\")\n",
    "        # Iterate through id -> timespan entries\n",
    "        # Combine multiple timespans into single\n",
    "        for article_id, article_dates in id_dates.iteritems():\n",
    "            start = min(article_dates)\n",
    "            end = max(article_dates)\n",
    "            # Get existing list of projects \n",
    "            try:\n",
    "                project_times = id_projects[article_id]\n",
    "            except KeyError:\n",
    "                project_times = {}\n",
    "                id_projects[article_id] = project_times\n",
    "            project_times[project_id] = [start, end]\n",
    "    log.info(\"Writing output\")\n",
    "    with open(exp.get_filename(out_file), \"wb\") as f:\n",
    "        f.write(msgpack.packb(id_projects))\n",
    "    log.info(\"Complete, cleaning up...\")\n",
    "except:\n",
    "    log.error(str(sys.exc_info()))\n",
    "    raise\n",
    "finally:\n",
    "    skipped_assess.close()\n",
    "    skipped_timespan.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
