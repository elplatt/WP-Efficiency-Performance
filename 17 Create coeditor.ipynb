{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "import os.path\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "import msgpack\n",
    "import sqlalchemy\n",
    "from sqlalchemy import desc, func, and_\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import database\n",
    "from database.schema import ArticleContributor, contributor_table, revision_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_name=\"17_create_coeditor\"\n",
    "project_file = \"data/projects-2016-10-14-dedup.json\"\n",
    "affiliation_file = \"%d-article-editor.tsv\"\n",
    "coeditor_file = \"%d-coeditor.tsv\"\n",
    "coeditor_mp = \"%d-coeditor.mp\"\n",
    "batch_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_ids = []\n",
    "with open(project_file, \"rb\") as f:\n",
    "    for row in f:\n",
    "        data = json.loads(row)\n",
    "        project_ids.append(data[\"project_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load project data into articles_contributors\n",
    "def compute_articles_contributors(project_id, log):\n",
    "    start = time.time()\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=database.engine)\n",
    "    session = Session()\n",
    "    try:\n",
    "        revisions = revision_table(project_id)\n",
    "        row_count = 0\n",
    "        to_add = []\n",
    "        log.info(\"Computing articles_contributors for project %d\" % project_id)\n",
    "        for result in session.query(\n",
    "                revisions.article_id,\n",
    "                revisions.contributor_id,\n",
    "                func.min(revisions.timestamp),\n",
    "                func.max(revisions.timestamp)) \\\n",
    "                .group_by(revisions.article_id, revisions.contributor_id) \\\n",
    "                .filter(and_(\n",
    "                    revisions.contributor_id != 0,\n",
    "                    revisions.contributor_id != None)):\n",
    "            to_add.append(ArticleContributor(\n",
    "                article_id=result[0]\n",
    "                , contributor_id=result[1]\n",
    "                , first_edit=result[2]\n",
    "                , last_edit=result[3]))\n",
    "            row_count += 1\n",
    "            if len(to_add) >= batch_size:\n",
    "                session.add_all(to_add)\n",
    "                session.commit()\n",
    "                to_add = []\n",
    "                log.info(\"%d rows in %f seconds\" % (row_count, time.time() - start))\n",
    "                time.sleep(0.1)\n",
    "        session.add_all(to_add)\n",
    "        session.commit()\n",
    "        to_add = []\n",
    "        log.info(\"%d rows in %f seconds\" % (row_count, time.time() - start))\n",
    "    except:\n",
    "        log.error(sys.exc_info()[1])\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "        log.info(\"Finished computing project %d articles_contributors %d rows in %f seconds\" % (\n",
    "            project_id, row_count, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create indexes on articles_contributors\n",
    "def index_articles_contributors(project_id, log):\n",
    "    start = time.time()\n",
    "    log.info(\"Indexing articles_contributors\")\n",
    "    sql = \"CREATE INDEX last ON articles_contributors (last_edit);\"\n",
    "    database.engine.execute(sql)\n",
    "    sql = \"CREATE INDEX contributor ON articles_contributors (contributor_id);\"\n",
    "    database.engine.execute(sql)\n",
    "    log.info(\"Finished indexing project %d articles_contributors. %f seconds\" % (\n",
    "            project_id, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear articles_contributors\n",
    "def clear_articles_contributors(log):\n",
    "    log.info(\"Removing indexes from articles_contributors\")\n",
    "    sql = \"DROP INDEX last ON articles_contributors;\"\n",
    "    database.engine.execute(sql)\n",
    "    sql = \"DROP INDEX contributor ON articles_contributors;\"\n",
    "    database.engine.execute(sql)\n",
    "    log.info(\"Truncating articles_contributors\")\n",
    "    sql = \"TRUNCATE articles_contributors\"\n",
    "    database.engine.execute(sql)\n",
    "    log.info(\"Done clearing articles_contributors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clear <project_id>_contributor_contributor\n",
    "def clear_coeditor(project_id, log):\n",
    "    log.info(\"Truncating %d_contributor_contributor\" % project_id)\n",
    "    sql = \"TRUNCATE %d_contributor_contributor\" % project_id\n",
    "    database.engine.execute(sql)\n",
    "    log.info(\"Done clearing %d_contributor_contributor\" % project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write articles_contributors as a graph edge TSV\n",
    "def write_articles_contributors_tsv(project_id, exp):\n",
    "    start = time.time()\n",
    "    log = exp.get_logger()\n",
    "    log.info(\"Writing article-editor TSV for project %d\" % project_id)\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=database.engine)\n",
    "    session = Session()\n",
    "    row_count = 0\n",
    "    try:\n",
    "        fname = exp.get_filename(affiliation_file % project_id)\n",
    "        with open(fname, \"wb\") as f:\n",
    "            # Use yield_per to have sqlalchemy internally batch the results.\n",
    "            # This also requires setting .enable_eagerloads to False.\n",
    "            q = session.query(ArticleContributor) \\\n",
    "                .yield_per(10000) \\\n",
    "                .enable_eagerloads(False)\n",
    "            for result in q:\n",
    "                f.write((u\"%d\\t%s\\n\" % (result.contributor_id, result.article_id)).encode(\"utf-16-le\"))\n",
    "                row_count += 1\n",
    "    finally:\n",
    "        session.close()\n",
    "        log.info(\"Finished writing article-editor TSV for project %d. %d rows in %f seconds\" % (\n",
    "            project_id, row_count, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write <project_id>_contributor_contributor as a graph edge TSV\n",
    "def write_coeditor_tsv(project_id, exp):\n",
    "    start = time.time()\n",
    "    log = exp.get_logger()\n",
    "    log.info(\"Writing coeditor TSV for project %d\" % project_id)\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=database.engine)\n",
    "    session = Session()\n",
    "    row_count = 0\n",
    "    try:\n",
    "        fname = exp.get_filename(coeditor_file % project_id)\n",
    "        with open(fname, \"wb\") as f:\n",
    "            table = contributor_table(project_id)\n",
    "            # Use yield_per to have sqlalchemy internally batch the results.\n",
    "            # This also requires setting .enable_eagerloads to False.\n",
    "            q = session.query(table) \\\n",
    "                .yield_per(10000) \\\n",
    "                .enable_eagerloads(False)\n",
    "            f.write(u\"source_id\\ttarget_id\\n\".encode(\"utf-16-le\"))\n",
    "            for result in q:\n",
    "                f.write((u\"%d\\t%d\\n\" % (result.source_id, result.target_id)).encode(\"utf-16-le\"))\n",
    "                row_count += 1\n",
    "    finally:\n",
    "        session.close()\n",
    "        log.info(\"Finished writing coeditor TSV for project %d. %d rows in %f seconds\" % (\n",
    "            project_id, row_count, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an editor-editor network from articles_contributors\n",
    "# First, get all contributors to use as source nodes.\n",
    "# Then get keep track of a list of revisions by each contributor\n",
    "# and a list of all contributions by article, sorted by time.\n",
    "# For each (contributor, article) go through all contributions for that article after\n",
    "# the contributor's first edit, and add a directed link.\n",
    "def compute_coeditor(project_id, log, batch_size=100000):\n",
    "    log.info(\"Computing coeditor network for project %d\" % project_id)\n",
    "    start = time.time()\n",
    "    Session = sessionmaker()\n",
    "    Session.configure(bind=database.engine)\n",
    "    session = Session()\n",
    "    conn = database.engine.connect()\n",
    "    edges_complete = 0\n",
    "    edges_to_insert = list()\n",
    "    mp = open(exp.get_filename(coeditor_mp % project_id), \"wb\")\n",
    "    try:\n",
    "        # Map contributors to articles and first/last edits\n",
    "        ac_by_contributor = {}\n",
    "        # Map articles to a list of contributors sorted by last edit\n",
    "        ac_sorted_last_by_article = {}\n",
    "        # Get all contributors in this project\n",
    "        log.info(\"  Querying sources from articles_contributors\");\n",
    "        sources = session.query(ArticleContributor.contributor_id).distinct().all()\n",
    "        # Populate maps\n",
    "        log.info(\"  Querying data from articles_contributors\");\n",
    "        ac_q = session.query(\n",
    "                ArticleContributor.contributor_id,\n",
    "                ArticleContributor.article_id,\n",
    "                ArticleContributor.first_edit,\n",
    "                ArticleContributor.last_edit) \\\n",
    "            .order_by(desc(ArticleContributor.last_edit))\n",
    "        ac_all = ac_q.all()\n",
    "        log.info(\"  Creating article/contributor lookup tables\");\n",
    "        for row in ac_all:\n",
    "            try:\n",
    "                ac_by_contributor[int(row[0])].append(row)\n",
    "            except KeyError:\n",
    "                ac_by_contributor[int(row[0])] = [row]\n",
    "            try:\n",
    "                ac_sorted_last_by_article[row[1]].append(row)\n",
    "            except KeyError:\n",
    "                ac_sorted_last_by_article[row[1]] = [row]\n",
    "        log.info(\"  Creating edges for each source\");\n",
    "        for i, source in enumerate(sources):\n",
    "            source_id = int(source[0])\n",
    "            source_edges = set()\n",
    "            for source_ac in ac_by_contributor[source_id]:\n",
    "                article_id = source_ac[1]\n",
    "                source_first_edit = source_ac[2]\n",
    "                for target_ac in ac_sorted_last_by_article[article_id]:\n",
    "                    target_last_edit = target_ac[3]\n",
    "                    if source_first_edit >= target_last_edit:\n",
    "                        # The targets are sorted in descending order of last edit.\n",
    "                        # Any later rows will be earlier so we can go to the next article\n",
    "                        break\n",
    "                    target_id = int(target_ac[0])\n",
    "                    if target_id != source_id:\n",
    "                        source_edges.add( (source_id, target_id) )\n",
    "            # Write to MessagePack file\n",
    "            target_list = [edge[1] for edge in source_edges]\n",
    "            mp_row = (source, target_list)\n",
    "            mp.write(msgpack.packb(mp_row))\n",
    "            del target_list\n",
    "            del mp_row\n",
    "            # Load edges into contributor_contributor in batches\n",
    "            coeditors = contributor_table(project_id).__table__\n",
    "            source_edge_list = list(source_edges)\n",
    "            edges_to_insert = edges_to_insert + source_edge_list\n",
    "            total_edges = len(edges_to_insert)\n",
    "            while len(edges_to_insert) > batch_size:\n",
    "                log.info(\"  Inserting edges (%d), %d sources complete\" % (total_edges, i))\n",
    "                batch = edges_to_insert[0:batch_size]\n",
    "                batch_data = []\n",
    "                for edge in batch:\n",
    "                    batch_data.append({\"source_id\": edge[0], \"target_id\": edge[1]})\n",
    "                conn.execute(coeditors.insert(), batch_data)\n",
    "                edges_to_insert = edges_to_insert[batch_size:]\n",
    "                edges_complete += len(batch_data)\n",
    "        # Insert remaining edges\n",
    "        total_edges = len(edges_to_insert)\n",
    "        while len(edges_to_insert) > 0:\n",
    "            log.info(\"  Inserting remaining edges\")\n",
    "            batch = edges_to_insert[0:batch_size]\n",
    "            batch_data = []\n",
    "            for edge in batch:\n",
    "                batch_data.append({\"source_id\": edge[0], \"target_id\": edge[1]})\n",
    "            conn.execute(coeditors.insert(), batch_data)\n",
    "            edges_to_insert = edges_to_insert[batch_size:]\n",
    "            edges_complete += len(batch_data)\n",
    "        log.info(\"Finished computing coeditor network for project %d. %d in %f seconds\" % (\n",
    "            project_id, edges_complete, time.time() - start))\n",
    "    except:\n",
    "        log.error(sys.exc_info())\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "        conn.close()\n",
    "        mp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create indexes on contributors_contributors\n",
    "def index_coeditor(project_id, log):\n",
    "    start = time.time()\n",
    "    log.info(\"Indexing coeditor table\")\n",
    "    sql = \"CREATE INDEX cc%d_edge ON %d_contributor_contributor (source_id, target_id);\" % (\n",
    "        project_id, project_id)\n",
    "    database.engine.execute(sql)\n",
    "    log.info(\"Done indexing coeditor table for project %d. %f seconds\" % (\n",
    "        project_id, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanup = None\n",
    "skip_to = 1\n",
    "exp = logbook.Experiment(exp_name)\n",
    "log = exp.get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #index_articles_contributors(cleanup, log)\n",
    "    #write_articles_contributors_tsv(cleanup, log)\n",
    "    #compute_coeditor(cleanup, log)\n",
    "    #clear_articles_contributors(log)\n",
    "    for project_id in project_ids:\n",
    "        if project_id < skip_to:\n",
    "            continue\n",
    "        compute_articles_contributors(project_id, log)\n",
    "        index_articles_contributors(project_id, log)\n",
    "        compute_coeditor(project_id, log)\n",
    "        clear_articles_contributors(log)\n",
    "except:\n",
    "    log.error(\"error: %s\" % str(sys.exc_info()))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
