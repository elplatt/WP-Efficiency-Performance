{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import logging\n",
      "import time\n",
      "import os.path\n",
      "import sys\n",
      "from IPython.display import clear_output\n",
      "from matplotlib import pyplot as plt\n",
      "import msgpack\n",
      "import sqlalchemy\n",
      "from sqlalchemy import desc, func\n",
      "from sqlalchemy.orm import sessionmaker"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import database\n",
      "from database.schema import ArticleContributor, contributor_table, revision_table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exp_name=\"08_create_coeditor\"\n",
      "project_file = \"data/articles-projects.json\"\n",
      "affiliation_file = \"%d-article-editor.tsv\"\n",
      "coeditor_file = \"%d-coeditor.tsv\"\n",
      "coeditor_mp = \"%d-coeditor.mp\"\n",
      "batch_size = 50000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logbook"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "project_ids = []\n",
      "with open(project_file, \"rb\") as f:\n",
      "    for row in f:\n",
      "        data = json.loads(row)\n",
      "        project_ids.append(data[\"project_id\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load project data into articles_contributors\n",
      "def compute_articles_contributors(project_id, log):\n",
      "    start = time.time()\n",
      "    Session = sessionmaker()\n",
      "    Session.configure(bind=database.engine)\n",
      "    session = Session()\n",
      "    try:\n",
      "        revisions = revision_table(project_id)\n",
      "        row_count = 0\n",
      "        to_add = []\n",
      "        log.info(\"Computing articles_contributors for project %d\" % project_id)\n",
      "        for result in session.query(\n",
      "                revisions.contributor_id,\n",
      "                revisions.article_name,\n",
      "                func.min(revisions.timestamp),\n",
      "                func.max(revisions.timestamp)) \\\n",
      "                .group_by(revisions.contributor_id, revisions.article_name) \\\n",
      "                .filter(revisions.contributor_id != 0):\n",
      "            to_add.append(ArticleContributor(\n",
      "                contributor_id=result[0]\n",
      "                , article_name=result[1]\n",
      "                , first_edit=result[2]\n",
      "                , last_edit=result[3]))\n",
      "            row_count += 1\n",
      "            if len(to_add) >= batch_size:\n",
      "                session.add_all(to_add)\n",
      "                session.commit()\n",
      "                to_add = []\n",
      "                log.info(\"%d rows in %f seconds\" % (row_count, time.time() - start))\n",
      "                time.sleep(0.1)\n",
      "        session.add_all(to_add)\n",
      "        session.commit()\n",
      "        to_add = []\n",
      "        log.info(\"%d rows in %f seconds\" % (row_count, time.time() - start))\n",
      "    except:\n",
      "        log.error(sys.exc_info()[1])\n",
      "        raise\n",
      "    finally:\n",
      "        session.close()\n",
      "        log.info(\"Finished computing project %d articles_contributors %d rows in %f seconds\" % (\n",
      "            project_id, row_count, time.time() - start))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create indexes on articles_contributors\n",
      "def index_articles_contributors(project_id, log):\n",
      "    start = time.time()\n",
      "    log.info(\"Indexing articles_contributors\")\n",
      "    sql = \"CREATE INDEX articles_contributors_first ON articles_contributors (first_edit);\"\n",
      "    database.engine.execute(sql)\n",
      "    sql = \"CREATE INDEX articles_contributors_last ON articles_contributors (last_edit);\"\n",
      "    database.engine.execute(sql)\n",
      "    sql = \"CREATE INDEX articles_contributors_name ON articles_contributors (article_name);\"\n",
      "    database.engine.execute(sql)\n",
      "    log.info(\"Finished indexing project %d articles_contributors. %f seconds\" % (\n",
      "            project_id, time.time() - start))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Clear articles_contributors\n",
      "def clear_articles_contributors(log):\n",
      "    log.info(\"Removing indexes from articles_contributors\")\n",
      "    sql = \"DROP INDEX articles_contributors_first ON articles_contributors;\"\n",
      "    database.engine.execute(sql)\n",
      "    sql = \"DROP INDEX articles_contributors_last ON articles_contributors;\"\n",
      "    database.engine.execute(sql)\n",
      "    sql = \"DROP INDEX articles_contributors_name ON articles_contributors;\"\n",
      "    database.engine.execute(sql)\n",
      "    log.info(\"Truncating articles_contributors\")\n",
      "    sql = \"TRUNCATE articles_contributors\"\n",
      "    database.engine.execute(sql)\n",
      "    log.info(\"Done clearing articles_contributors\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Clear <project_id>_contributor_contributor\n",
      "def clear_coeditor(project_id, log):\n",
      "    log.info(\"Truncating %d_contributor_contributor\" % project_id)\n",
      "    sql = \"TRUNCATE %d_contributor_contributor\" % project_id\n",
      "    database.engine.execute(sql)\n",
      "    log.info(\"Done clearing %d_contributor_contributor\" % project_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write articles_contributors as a graph edge TSV\n",
      "def write_articles_contributors_tsv(project_id, exp):\n",
      "    start = time.time()\n",
      "    log = exp.get_logger()\n",
      "    log.info(\"Writing article-editor TSV for project %d\" % project_id)\n",
      "    Session = sessionmaker()\n",
      "    Session.configure(bind=database.engine)\n",
      "    session = Session()\n",
      "    row_count = 0\n",
      "    try:\n",
      "        fname = exp.get_filename(affiliation_file % project_id)\n",
      "        with open(fname, \"wb\") as f:\n",
      "            # Use yield_per to have sqlalchemy internally batch the results.\n",
      "            # This also requires setting .enable_eagerloads to False.\n",
      "            q = session.query(ArticleContributor) \\\n",
      "                .yield_per(10000) \\\n",
      "                .enable_eagerloads(False)\n",
      "            for result in q:\n",
      "                f.write((u\"%d\\t%s\\n\" % (result.contributor_id, result.article_name)).encode(\"utf-16-le\"))\n",
      "                row_count += 1\n",
      "    finally:\n",
      "        session.close()\n",
      "        log.info(\"Finished writing article-editor TSV for project %d. %d rows in %f seconds\" % (\n",
      "            project_id, row_count, time.time() - start))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write <project_id>_contributor_contributor as a graph edge TSV\n",
      "def write_coeditor_tsv(project_id, exp):\n",
      "    start = time.time()\n",
      "    log = exp.get_logger()\n",
      "    log.info(\"Writing coeditor TSV for project %d\" % project_id)\n",
      "    Session = sessionmaker()\n",
      "    Session.configure(bind=database.engine)\n",
      "    session = Session()\n",
      "    row_count = 0\n",
      "    try:\n",
      "        fname = exp.get_filename(coeditor_file % project_id)\n",
      "        with open(fname, \"wb\") as f:\n",
      "            table = contributor_table(project_id)\n",
      "            # Use yield_per to have sqlalchemy internally batch the results.\n",
      "            # This also requires setting .enable_eagerloads to False.\n",
      "            q = session.query(table) \\\n",
      "                .yield_per(10000) \\\n",
      "                .enable_eagerloads(False)\n",
      "            f.write(u\"source_id\\ttarget_id\\n\".encode(\"utf-16-le\"))\n",
      "            for result in q:\n",
      "                f.write((u\"%d\\t%d\\n\" % (result.source_id, result.target_id)).encode(\"utf-16-le\"))\n",
      "                row_count += 1\n",
      "    finally:\n",
      "        session.close()\n",
      "        log.info(\"Finished writing coeditor TSV for project %d. %d rows in %f seconds\" % (\n",
      "            project_id, row_count, time.time() - start))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create an editor-editor network from articles_contributors\n",
      "def compute_coeditor(project_id, log, batch_size=100000):\n",
      "    log.info(\"Computing coeditor network for project %d\" % project_id)\n",
      "    start = time.time()\n",
      "    Session = sessionmaker()\n",
      "    Session.configure(bind=database.engine)\n",
      "    session = Session()\n",
      "    conn = database.engine.connect()\n",
      "    edges_complete = 0\n",
      "    edges_to_insert = list()\n",
      "    mp = open(exp.get_filename(coeditor_mp % project_id), \"wb\")\n",
      "    try:\n",
      "        # Map contributors to articles and first/last edits\n",
      "        ac_by_contributor = {}\n",
      "        # Map articles to a list of contributors sorted by last edit\n",
      "        ac_sorted_last_by_article = {}\n",
      "        # Get all contributors in this project\n",
      "        log.info(\"  Querying sources from articles_contributors\");\n",
      "        sources = session.query(ArticleContributor.contributor_id).distinct().all()\n",
      "        # Populate maps\n",
      "        log.info(\"  Querying data from articles_contributors\");\n",
      "        ac_q = session.query(\n",
      "                ArticleContributor.contributor_id,\n",
      "                ArticleContributor.article_name,\n",
      "                ArticleContributor.first_edit,\n",
      "                ArticleContributor.last_edit) \\\n",
      "            .order_by(desc(ArticleContributor.last_edit))\n",
      "        ac_all = ac_q.all()\n",
      "        log.info(\"  Creating article/contributor lookup tables\");\n",
      "        for row in ac_all:\n",
      "            try:\n",
      "                ac_by_contributor[int(row[0])].append(row)\n",
      "            except KeyError:\n",
      "                ac_by_contributor[int(row[0])] = [row]\n",
      "            try:\n",
      "                ac_sorted_last_by_article[row[1]].append(row)\n",
      "            except KeyError:\n",
      "                ac_sorted_last_by_article[row[1]] = [row]\n",
      "        log.info(\"  Creating edges for each source\");\n",
      "        for i, source in enumerate(sources):\n",
      "            source_id = int(source[0])\n",
      "            source_edges = set()\n",
      "            for source_ac in ac_by_contributor[source_id]:\n",
      "                article_name = source_ac[1]\n",
      "                source_first_edit = source_ac[2]\n",
      "                for target_ac in ac_sorted_last_by_article[article_name]:\n",
      "                    target_last_edit = target_ac[3]\n",
      "                    if source_first_edit >= target_last_edit:\n",
      "                        # The targets are sorted in descending order of last edit.\n",
      "                        # Any later rows will be earlier so we can go to the next article\n",
      "                        break\n",
      "                    target_id = int(target_ac[0])\n",
      "                    if target_id != source_id:\n",
      "                        source_edges.add( (source_id, target_id) )\n",
      "            # Write to MessagePack file\n",
      "            target_list = [edge[1] for edge in source_edges]\n",
      "            mp_row = (source, target_list)\n",
      "            mp.write(msgpack.packb(mp_row))\n",
      "            del target_list\n",
      "            del mp_row\n",
      "            # Load edges into contributor_contributor in batches\n",
      "            coeditors = contributor_table(project_id).__table__\n",
      "            source_edge_list = list(source_edges)\n",
      "            edges_to_insert = edges_to_insert + source_edge_list\n",
      "            total_edges = len(edges_to_insert)\n",
      "            while len(edges_to_insert) > batch_size:\n",
      "                log.info(\"  Inserting edges (%d), %d sources complete\" % (total_edges, i))\n",
      "                batch = edges_to_insert[0:batch_size]\n",
      "                batch_data = []\n",
      "                for edge in batch:\n",
      "                    batch_data.append({\"source_id\": edge[0], \"target_id\": edge[1]})\n",
      "                conn.execute(coeditors.insert(), batch_data)\n",
      "                edges_to_insert = edges_to_insert[batch_size:]\n",
      "                edges_complete += len(batch_data)\n",
      "        # Insert remaining edges\n",
      "        total_edges = len(edges_to_insert)\n",
      "        while len(edges_to_insert) > 0:\n",
      "            log.info(\"  Inserting remaining edges\")\n",
      "            batch = edges_to_insert[0:batch_size]\n",
      "            batch_data = []\n",
      "            for edge in batch:\n",
      "                batch_data.append({\"source_id\": edge[0], \"target_id\": edge[1]})\n",
      "            conn.execute(coeditors.insert(), batch_data)\n",
      "            edges_to_insert = edges_to_insert[batch_size:]\n",
      "            edges_complete += len(batch_data)\n",
      "        log.info(\"Finished computing coeditor network for project %d. %d in %f seconds\" % (\n",
      "            project_id, edges_complete, time.time() - start))\n",
      "    except:\n",
      "        log.error(sys.exc_info())\n",
      "        raise\n",
      "    finally:\n",
      "        session.close()\n",
      "        conn.close()\n",
      "        mp.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create indexes on contributors_contributors\n",
      "def index_coeditor(project_id, log):\n",
      "    start = time.time()\n",
      "    log.info(\"Indexing coeditor table\")\n",
      "    sql = \"CREATE INDEX cc%d_edge ON %d_contributor_contributor (source_id, target_id);\" % (\n",
      "        project_id, project_id)\n",
      "    database.engine.execute(sql)\n",
      "    log.info(\"Done indexing coeditor table for project %d. %f seconds\" % (\n",
      "        project_id, time.time() - start))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleanup = 490\n",
      "skip_to = 491\n",
      "exp = logbook.Experiment(exp_name)\n",
      "log = exp.get_logger()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    #index_articles_contributors(cleanup, log)\n",
      "    #write_articles_contributors_tsv(cleanup, log)\n",
      "    #compute_coeditor(cleanup, log)\n",
      "    clear_articles_contributors(log)\n",
      "    for project_id in range(skip_to, 2037):\n",
      "        compute_articles_contributors(project_id, log)\n",
      "        index_articles_contributors(project_id, log)\n",
      "        write_articles_contributors_tsv(project_id, exp)\n",
      "        compute_coeditor(project_id, log)\n",
      "        clear_articles_contributors(log)\n",
      "except:\n",
      "    log.error(\"error: %s\" % str(sys.exc_info()))\n",
      "    raise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}