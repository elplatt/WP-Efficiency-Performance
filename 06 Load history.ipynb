{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import calendar\n",
      "import csv\n",
      "import datetime\n",
      "import dateutil.parser\n",
      "import json\n",
      "import sys\n",
      "import time\n",
      "import sqlalchemy\n",
      "from sqlalchemy.exc import IntegrityError\n",
      "from sqlalchemy.orm import sessionmaker"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "project_file = \"data/articles-projects.json\"\n",
      "history_file = \"data/history_output.csv\"\n",
      "log_file = \"data/06_load_history.log\"\n",
      "batch_size = 10000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import database\n",
      "from database.schema import (\n",
      "    article_project,\n",
      "    article_project_names,\n",
      "    revision_table\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "revision_shards = {}\n",
      "projects = {}\n",
      "projects_by_name = {}\n",
      "with open(project_file, 'rb') as f:\n",
      "    for i, row in enumerate(f):\n",
      "        data = json.loads(row)\n",
      "        project_id = data['project_id']\n",
      "        projects[project_id] = data\n",
      "        projects[project_id]['articles'] = set(projects[project_id]['articles'])\n",
      "        projects_by_name[data[\"project_name\"]] = project_id\n",
      "        t = revision_table(project_id)\n",
      "        revision_shards[project_id] = t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/sqlalchemy/ext/declarative/clsregistry.py:159: SAWarning: This declarative base already contains a class with the same class name and module name as database.schema.ProjectRevision, and will be replaced in the string-lookup table.\n",
        "  existing.add_item(cls)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Session = sessionmaker()\n",
      "Session.configure(bind=database.engine)\n",
      "session = Session()\n",
      "with open(log_file, 'wb') as log:\n",
      "    try:\n",
      "        for project_id in projects.iterkeys():\n",
      "            last = -1\n",
      "            project_name = projects[project_id]['project_name']\n",
      "            print('Beginning project %d: %s\\n' % (project_id, project_name))\n",
      "            sys.stdout.flush()\n",
      "            log.write('Beginning project\\t%d\\t%s\\n' % (project_id, project_name))\n",
      "            articles_names = set(projects[project_id]['articles'])\n",
      "            article_ids = set()\n",
      "            print(\"Loading article ids\")\n",
      "            sys.stdout.flush()\n",
      "            start = time.time()\n",
      "            for article_id in session.query(article_project.c.article_id) \\\n",
      "                    .filter(article_project.c.project_id == project_id).all():\n",
      "                article_ids.add(article_id[0])\n",
      "            print \"Finished loading %d ids in %f seconds\" % (len(article_ids), time.time() - start)\n",
      "            to_insert = []\n",
      "            print(\"Opening history file\");\n",
      "            sys.stdout.flush()\n",
      "            start = time.time()\n",
      "            with open(history_file, 'rb') as f:\n",
      "                skipped = []\n",
      "                reader = csv.reader(f)\n",
      "                for i, row in enumerate(reader):\n",
      "                    last = i\n",
      "                    if i % 100000 == 0:\n",
      "                        log.write(\"Rows processed\\t%d\\t%f\\n\" % (i, time.time() - start))\n",
      "                        print \"%d rows processed in %f seconds\" % (i, time.time() - start)\n",
      "                        sys.stdout.flush()\n",
      "                    try:\n",
      "                        (article_name\n",
      "                         , article_namespace\n",
      "                         , article_id\n",
      "                         , redirect\n",
      "                         , revision_num\n",
      "                         , revision_id\n",
      "                         , timestamp\n",
      "                         , contributor_name\n",
      "                         , contributor_id\n",
      "                         , minor\n",
      "                         , comment\n",
      "                         , bytes\n",
      "                         , bytes_diff) = row\n",
      "                        ts = dateutil.parser.parse(timestamp)\n",
      "                        ts = datetime.datetime.fromtimestamp(calendar.timegm(ts.timetuple()))\n",
      "                        if contributor_id == '':\n",
      "                            contributor_id = 0\n",
      "                        if minor == '1':\n",
      "                            minor = True\n",
      "                        else:\n",
      "                            minor = False\n",
      "                        if article_id is not '' and article_id is not '-1':\n",
      "                            if int(article_id.strip()) in article_ids:\n",
      "                                to_insert.append(\n",
      "                                    revision_shards[project_id](\n",
      "                                        article_name = article_name\n",
      "                                         , article_id = article_id\n",
      "                                         , redirect = redirect\n",
      "                                         , revision_num = revision_num\n",
      "                                         , revision_id = revision_id\n",
      "                                         , timestamp = ts\n",
      "                                         , contributor_id = contributor_id\n",
      "                                         , minor = minor\n",
      "                                         , comment = comment\n",
      "                                         , diff_bytes = bytes_diff\n",
      "                                    )\n",
      "                                )\n",
      "                        else:\n",
      "                            if article_name in article_names:\n",
      "                                to_insert.append(\n",
      "                                    revision_shards[project_id](\n",
      "                                        article_name = article_name\n",
      "                                         , article_id = -1\n",
      "                                         , redirect = redirect\n",
      "                                         , revision_num = revision_num\n",
      "                                         , revision_id = revision_id\n",
      "                                         , timestamp = ts\n",
      "                                         , contributor_id = contributor_id\n",
      "                                         , minor = minor\n",
      "                                         , comment = comment\n",
      "                                         , diff_bytes = bytes_diff\n",
      "                                    )\n",
      "                                )\n",
      "                        if len(to_insert) >= batch_size:\n",
      "                            print \"Commiting %d rows\" % len(to_insert)\n",
      "                            session.add_all(to_insert)\n",
      "                            session.commit()\n",
      "                            to_insert = []\n",
      "                    except ValueError:\n",
      "                        print(\"Skipped row\\t%d\\t%s\\n\" % (i, sys.exc_info()))\n",
      "                        log.write(\"Skipped row\\t%d\\t%s\\n\" % (i, sys.exc_info()))\n",
      "                        continue\n",
      "                print \"Commiting %d rows\" % len(to_insert)\n",
      "                session.add_all(to_insert)\n",
      "                session.commit()\n",
      "                to_insert = []\n",
      "    except:\n",
      "        log.write(\"Exception on row\\t%d\\t%s\\n\" % (last, sys.exc_info()))\n",
      "        session.close()\n",
      "        raise\n",
      "session.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Beginning project 309: Wikipedia:WikiProject Sociology\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading article ids\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished loading 4803 ids in 326.174827 seconds\n",
        "Opening history file\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 rows processed in 0.060812 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 10000 rows\n",
        "100000 rows processed in 23.211749 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200000 rows processed in 41.223475 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "300000 rows processed in 59.204633 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "400000 rows processed in 77.185543 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 10000 rows\n",
        "500000 rows processed in 97.695188 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "600000 rows processed in 115.695562 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "700000 rows processed in 133.299508 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "800000 rows processed in 150.899878 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "900000 rows processed in 168.789925 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000000 rows processed in 186.431140 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1100000 rows processed in 204.024387 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1200000 rows processed in 221.605507 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1300000 rows processed in 239.046169 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1400000 rows processed in 256.507298 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1500000 rows processed in 273.370869 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1600000 rows processed in 290.199580 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1700000 rows processed in 307.034964 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1800000 rows processed in 323.895283 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1900000 rows processed in 340.830365 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2000000 rows processed in 357.698038 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2100000 rows processed in 374.562868 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2200000 rows processed in 391.552097 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2300000 rows processed in 408.813148 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2400000 rows processed in 425.594396 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2500000 rows processed in 442.600813 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2600000 rows processed in 460.077948 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2700000 rows processed in 477.081762 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2800000 rows processed in 494.021740 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2900000 rows processed in 510.944682 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3000000 rows processed in 527.908814 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3100000 rows processed in 544.896154 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3200000 rows processed in 561.867115 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3300000 rows processed in 578.832603 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3400000 rows processed in 595.762603 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3500000 rows processed in 612.734174 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3600000 rows processed in 629.797817 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 10000 rows\n",
        "3700000 rows processed in 650.690536 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3800000 rows processed in 667.298661 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 10000 rows\n",
        "3900000 rows processed in 686.903940 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 10000 rows\n",
        "4000000 rows processed in 707.645193 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4100000 rows processed in 724.477772 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 10000 rows\n",
        "4200000 rows processed in 743.631195 seconds"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4300000 rows processed in 760.717285 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4400000 rows processed in 777.673138 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4500000 rows processed in 794.778139 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4600000 rows processed in 811.766308 seconds\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Commiting 4526 rows\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "session.commit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}